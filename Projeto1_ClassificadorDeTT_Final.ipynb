{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Barbara Martins Damasceno\n",
    "\n",
    "Nome: Daniel Costa Delattre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Sobre o projeto\n",
    "Como projeto 1, escolhemos como tema **Mercado Financeiro** para desenvolver um classificador de tweets a partir do conhecimento adquirido durante as aulas da disciplina **Ciências dos Dados**. \n",
    "\n",
    "### Primeiros passos\n",
    "\n",
    "Para coletar os tweets, usamos o arquivo []nome e pedimos para que coletasse 300 tweets sobre a palavra mercado para a folha Treinamento e outros 150 para a folha Teste da planilha t1.xlsx. Em seguida fizemos a classificação manual de quais tweets eram relevantes e quais eram irrelevantes. \n",
    "\n",
    "\n",
    "## Construindo o classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, csv, string, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando o diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\User\\Documents\\2 Semestre ENG\\Ciência dos Dados\\cdadosproj1\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_excel('t1.xlsx', sheet_name='Treinamento1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instrução:** Faça aqui uma descrição do seu produto e o que considerou como relevante ou não relevante na classificação dos tweets.\n",
    "\n",
    "Como dito anteriormente, esse projeto se trata de um classificador de tweets que tem como assunto o **Mercado Financeiro**. Durante a classificação dos tweets sobre **mercado** classificamos em duas categorias: Relevante e Irrelevante.\n",
    "\n",
    "  ### Relevantes\n",
    "   - Situação da bolsa de valores\n",
    "   - Preço do dólar\n",
    "   - Comentários sobre o circuit break\n",
    "   - Situação da economia\n",
    "   - Comentário sobre o mercado financeiro (compra de ações, situação, etc)\n",
    "   \n",
    "  ### Irrelevantes\n",
    "   - Tweets sobre compras no supermercado, ou situação que aconteceram nele\n",
    "   - Comentários totalmente políticos\n",
    "   - Relacionados ao Mercado Livre\n",
    "   - Mercado de trabalho\n",
    "   - Vendas\n",
    "   - Mercado como ramo (mercado do esporte)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "**Instrução:** Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de realizar os passos para o desenvolvimento do classificador, foi escrita uma função para limpar cada um dos tweets coletados. A limpeza consiste em trocar certos caracteres e por espaço, além de excluir todos os links externos. Essa função será extremamente necessária para criar um banco de informações com apenas palavras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    punctuation = '[!-.:?;@/…|⠀_=♥️\"]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    text_s = re.sub(r\"http\\S+\", \"\", text_subbed)\n",
    "    ttt = re.sub(r'''((?:https?://|www\\d{0,3}[.]|[a-z0-9.-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|(([^\\s()<>]+|(([^\\s()<>]+)))))+(?:(([^\\s()<>]+|(([^\\s()<>]+))))|[^\\s`!()[]{};:'\".,<>?«»“”‘’]))''', \" \", text_s)\n",
    "    return ttt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voltando ao classificador, ocorreu a transformação da coluna Relevância como categoria e renomeando os níveis pois se trata de uma variável qualitativa em que a escala de números não é a forma mais adequada para interpretar uma informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevância</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nunca vi um mercado tão cheio</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rt @folha: guedes diz que vai acionar stf cont...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>deixando tudo mais barato:\\ncupom ifood (app) ...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>rt @reporterlacerda: inovação no grenal\\n\\npre...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@ericat_lol mercado a vista vai ficar parado p...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                                        Treinamento   relevância\n",
       "0    0                      nunca vi um mercado tão cheio  Irrelevante\n",
       "1    1  rt @folha: guedes diz que vai acionar stf cont...    Relevante\n",
       "2    2  deixando tudo mais barato:\\ncupom ifood (app) ...  Irrelevante\n",
       "3    3  rt @reporterlacerda: inovação no grenal\\n\\npre...  Irrelevante\n",
       "4    4  @ericat_lol mercado a vista vai ficar parado p...    Relevante"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.loc[:,'relevância'] = dados['relevância'].astype('category')\n",
    "dados.relevância.cat.categories = ['Irrelevante', 'Relevante']\n",
    "dados.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o comando value_counts é possível verificar quais são os valores que cada nível de relevância representa do total de tweets em treinamento: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade de ser relevante: 0.59\n",
      "Probabilidade de ser irrelevante: 0.41\n"
     ]
    }
   ],
   "source": [
    "#Dividindo a porcentagem em relevante e irrelevante\n",
    "base = dados.relevância.value_counts(True, sort=False)\n",
    "irrelevante = base[0]\n",
    "relevante = base[1]\n",
    "print(f\"Probabilidade de ser relevante: {relevante}\")\n",
    "print(f\"Probabilidade de ser irrelevante: {irrelevante}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separa-se as informação dos tweets relevantes dos irrelavantes por meio da criação de variáveis que armazenam só os dados de determinado nível: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Novas folhas de dados criadas a partir da relevancia\n",
    "dados_r = dados[dados.relevância == 'Relevante'].copy()\n",
    "dados_i = dados[dados.relevância == 'Irrelevante'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a separação dos dados, inicia-se o processo de tratamento do texto dos tweets. Para isso realiza-se as seguintes etapas:\n",
    "\n",
    "1)Armazenar cada tweet como um item de uma lista;\n",
    "\n",
    "2)Usar a função cleanup (escrita no início do código) para limpar caracteres especiais e retirar links;\n",
    "\n",
    "3)Armazenar cada palavra desse tweet numa lista;\n",
    "\n",
    "4)Criar um pd.Series com a lista de palavras;\n",
    "\n",
    "5)Usa-se o comando value_counts para verificar a frequência de cada palavra dos tweets relevantes. Como por exemplo, quantas vezes aparecer a palavra mercado entre todos os tweets relevantes;\n",
    "\n",
    "6)Repete-se o processo para os irrelevantes também.\n",
    "\n",
    "Esse processo está nas 2 células abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 1\n",
    "l_ttr = dados_r.Treinamento.tolist()\n",
    "#pra cada palavra na lista do tt, vai criar uma lista só com todas as palavras\n",
    "l_ttr_split = []\n",
    "for i in l_ttr:\n",
    "    l_ttr_split.append(i.split())\n",
    "\n",
    "\n",
    "output=open('prt_rel.txt','w',encoding=\"utf-8-sig\")\n",
    "#lista com palavras limpas\n",
    "lpal_r = []\n",
    "\n",
    "for lista in l_ttr_split:\n",
    "    for pal in lista:\n",
    "        #Passo 2\n",
    "        c_pal = cleanup(pal)\n",
    "        if c_pal != \"\":\n",
    "            #Passo 3\n",
    "            lpal_r.append(c_pal)\n",
    "\n",
    "#Passo 4\n",
    "s_relev = pd.Series(lpal_r)\n",
    "#Passo 5\n",
    "t_relev = s_relev.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 6\n",
    "#tt em lista \n",
    "l_tti = dados_i.Treinamento.tolist()\n",
    "\n",
    "#pra cada palavra na lista do tt, vai criar uma lista só com todas as palavras\n",
    "l_tti_split = []\n",
    "for i in l_tti:\n",
    "    l_tti_split.append(i.split())\n",
    "\n",
    "output=open('prt_i.txt','w',encoding=\"utf-8-sig\")\n",
    "#lista com palavras limpas\n",
    "lpal_i = []\n",
    "for lista in l_tti_split:\n",
    "    for pal in lista:\n",
    "        c_pal = cleanup(pal)\n",
    "        if c_pal != \"\":\n",
    "            lpal_i.append(c_pal)\n",
    "            \n",
    "s_ire = pd.Series(lpal_i)\n",
    "t_ire = s_ire.value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, como descrito no título o classificador funcionará com base no Teorema de Naive-Bayes. Nesse projeto ele terá a mesmo estrutura do Naive-Bayes mas com o seguinte cenário:\n",
    "\n",
    "$$P(relevância|tweet)=\\frac{P(tweet \\cap relevância)}{P(tweet)}=\\frac{P(tweet|relevância)P(relevância)}{P(tweet)}\\hspace{3cm}\\mbox{}$$\n",
    "\n",
    "\n",
    "Mas, não será efetuada a divisão pela probabilidade dos tweet, por dois motivos:\n",
    "\n",
    "1 - A probabilidade será extremamente baixa devido a quantidade de termos (palavras do tweet) em relação ao tamanho do espaço amostral. \n",
    "\n",
    "2 - O interesse maior é na comparação entre os valores obtidos ao invés da probabilidade exata.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Calcula-se a probabilidade da relevância dado o tweet pelo: \n",
    " \n",
    "**1)** Produto das probabilidades de cada palavra que esse tweet contém\n",
    "\n",
    "**2)** Produto do item 1 pela probabilidade da relevância\n",
    "\n",
    "**3)** Comparação do item 2 entre as duas relevância ( P(tweet | relevância) > P(tweet | irrelevância) ?)\n",
    "\n",
    "**4)** Se a informação do item 3 é verdadeira, na coluna \"Classificador\" terá a informação 'Relevante', se falsa a informação será 'Irrelevante'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Além dos passos anteriores, dentro do código, algumas mudanças foram incrementadas devido a problemas observados. O primeiro é relacionado a situação: na análise de um tweet, uma ou mais palavras apareciam em um dos espaços amostrais (grupo de palavras dentre os tweets relevantes ou no grupo de palavras dentre os tweets irrelevantes) mas no próximo ela não existia, assim, o código não terminava de rodar ou zerava a probabilidade. Para resolver isso foi usado o método **Laplace Smoothing** que consiste na adição de uma probabilidade baixa a esse termo que zeraria, mas que não influencie na decisão do classificador. Então, para cada palavra \"nova\" foi adicionada a lista de palavras daquele tipo de relevância e a probabilidade de 1/(número de palavras encontradas naquela relevância). Porém, para não inviesar o banco de palavras daquele tipo de relevância, a cada troca de tweet é reinicializado o banco de palavras daquela relevância para o que foi encontrado anteriormente.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop para classificadar cada tweet da folha Treinamento\n",
    "for i in dados.num:\n",
    "    #Resetando o banco de dados de palavras para o inicial\n",
    "    lpal_i_temp = lpal_i\n",
    "    lpal_r_temp = lpal_r\n",
    "    #Capturando o tweet inteiro\n",
    "    tt = dados.iloc[i,1]\n",
    "    #Splitando o tweet, agora é uma lista com cada elemento sendo uma palavra\n",
    "    tt_split = tt.split() \n",
    "    #Cada elemento de frase é uma palavra do tweet já limpa\n",
    "    frase = []\n",
    "    for pal in tt_split:    \n",
    "        frase.append(cleanup(pal))\n",
    "        \n",
    "    \"Aplicação do Laplace Smoothing\"\n",
    "    #Verificando se há alguma palavra nova (fora das coletas pela folha Treinamento)\n",
    "    for plvr in frase:\n",
    "        if plvr not in lpal_i_temp:\n",
    "            #Se ela não estiver no grupo de palavras relevantes, ela é adicionada\n",
    "            lpal_i_temp.append(plvr)\n",
    "        if plvr not in lpal_r_temp:\n",
    "            #Se ela não estiver no grupo de palavras irrelevantes, ela é adicionada\n",
    "            lpal_r_temp.append(plvr)\n",
    "\n",
    "    #Atualiza séries e tabela de palavras relevantes e as irrelevantes também\n",
    "    s_relev_temp = pd.Series(lpal_r_temp)\n",
    "    t_relev_temp = s_relev_temp.value_counts(True)\n",
    "    \n",
    "    s_ire_temp = pd.Series(lpal_i_temp)\n",
    "    t_ire_temp = s_ire_temp.value_counts(True)\n",
    "    \n",
    "    #Total palavras\n",
    "    #Esse é o espaço amostral total usado\n",
    "    portug = lpal_i_temp + lpal_r_temp\n",
    "    s_prtg_temp = pd.Series(portug)\n",
    "    \n",
    "    #Recalcula probabilidade pelo número de palavras\n",
    "    p_relev = len(s_relev_temp)/len(s_prtg_temp)\n",
    "    p_irrelev = len(s_ire_temp)/len(s_prtg_temp)\n",
    "\n",
    "    try:\n",
    "        #Fazendo o produto da probabilidade de cada palavra dentre os relevantes e dos irrelevantes\n",
    "        pf_dado_r = t_relev_temp[frase].prod()\n",
    "        pf_dado_i = t_ire_temp[frase].prod()\n",
    "\n",
    "        #Calculando a probabibilidade de ser relevante ou irrelevante dado o tweet analisado\n",
    "        pr_dado_f = pf_dado_r * p_relev\n",
    "        pi_dado_f = pf_dado_i * p_irrelev\n",
    "        \n",
    "        #Comparando as probabilidades obtidas\n",
    "        if pr_dado_f > pi_dado_f:\n",
    "            if (pr_dado_f > pi_dado_f):\n",
    "                #Se for verdade, na coluna classificador é colocada a informação de Relevante aquele tweet\n",
    "                dados.loc[dados.num==i,'Classificador'] = 'Relevante'\n",
    "        else:\n",
    "            #Se não for verdade, na coluna classificador é colocada a informação de Irrelevante aquele tweet\n",
    "            dados.loc[dados.num==i,'Classificador'] = 'Irrelevante'     \n",
    "            \n",
    "    except:\n",
    "        print(\"erro frase \")\n",
    "        print(frase)\n",
    "        print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o comando crosstab, cruza-se as informações obtidas com o classificador e as informações reais. No caso, as informações reais é a classificação feita manualmente de tweets relevantes e irrelevantes na folha Treinamento do Excel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classificador</th>\n",
       "      <th>Irrelevante</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevância</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Irrelevante</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevante</th>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classificador  Irrelevante  Relevante\n",
       "relevância                           \n",
       "Irrelevante       0.400000   0.010000\n",
       "Relevante         0.023333   0.566667"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performace = pd.crosstab(dados['relevância'], dados['Classificador'], normalize=True)\n",
    "performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as informações da tabela acima é possível somar as porcentagens de acertos do classificador (Classificador:Irrelevante VS relevância:Irrelevante e Classificador:Relevante VS relevância:Relevante) e chegar na porcentagem de acertos totais, no caso, **96% de acertos**. Um valor alto na performace significa que nosso classificador é próximo do ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "**Instrução:** Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O princípio do loop é o mesmo do anterior mas os tweets analisados não são os mesmos, antes eram os tweets da folha Treinamento que serviu de base para a crianção do grupo de palavras de cada relevância; agora, será analisado cada tweet da folha Teste mas com a referência do grupo de palavras criado a partir da planilha Treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data da folha Teste\n",
    "dadosteste = pd.read_excel('t1.xlsx', sheet_name='Teste1')\n",
    "#Classificando como categoria \n",
    "dadosteste.loc[:,'relevância'] = dadosteste['relevância'].astype('category')\n",
    "#Renomeando os níveis da categoria\n",
    "dadosteste.relevância.cat.categories = ['Irrelevante', 'Relevante']\n",
    "\n",
    "#Loop para classificadar cada tweet da folha Teste\n",
    "for i in dadosteste.num:\n",
    "    #Resetando o banco de dados de palavras para o inicial\n",
    "    lpal_i_temp = lpal_i\n",
    "    lpal_r_temp = lpal_r\n",
    "    #Capturando o tweet inteiro\n",
    "    tt = dadosteste.iloc[i,1]\n",
    "    #Splitando o tweet, agora é uma lista com cada elemento sendo uma palavra\n",
    "    tt_split = tt.split() \n",
    "    #Cada elemento de frase é uma palavra do tweet já limpa\n",
    "    frase = []\n",
    "    for pal in tt_split:    \n",
    "        frase.append(cleanup(pal))\n",
    "        \n",
    "    \"Aplicação do Laplace Smoothing\"\n",
    "    #Verificando se há alguma palavra nova (fora daquelas coletas pela folha Treinamento)\n",
    "    for plvr in frase:\n",
    "        if plvr not in lpal_i_temp:\n",
    "            #Se ela não estiver no grupo de palavras relevantes, ela é adicionada\n",
    "            lpal_i_temp.append(plvr)\n",
    "        if plvr not in lpal_r_temp:\n",
    "            #Se ela não estiver no grupo de palavras irrelevantes, ela é adicionada\n",
    "            lpal_r_temp.append(plvr)\n",
    "\n",
    "    #Atualiza séries e tabela de palavras relevantes e as irrelevantes também\n",
    "    s_relev_temp = pd.Series(lpal_r_temp)\n",
    "    t_relev_temp = s_relev_temp.value_counts(True)\n",
    "    \n",
    "    s_ire_temp = pd.Series(lpal_i_temp)\n",
    "    t_ire_temp = s_ire_temp.value_counts(True)\n",
    "    \n",
    "    #Total palavras\n",
    "    #Esse é o espaço amostral total usado\n",
    "    portug = lpal_i_temp + lpal_r_temp\n",
    "    s_prtg_temp = pd.Series(portug)\n",
    "    \n",
    "    #Recalcula probabilidade pelo número de palavras\n",
    "    p_relev = len(s_relev_temp)/len(s_prtg_temp)\n",
    "    p_irrelev = len(s_ire_temp)/len(s_prtg_temp)\n",
    "\n",
    "    try:\n",
    "        #Fazendo o produto da probabilidade de cada palavra dentre os relevantes e dos irrelevantes\n",
    "        pf_dado_r = t_relev_temp[frase].prod()\n",
    "        pf_dado_i = t_ire_temp[frase].prod()\n",
    "\n",
    "        #Calculando a probabibilidade de ser relevante ou irrelevante dado o tweet analisado\n",
    "        pr_dado_f = pf_dado_r * p_relev\n",
    "        pi_dado_f = pf_dado_i * p_irrelev\n",
    "        \n",
    "        #Comparando as probabilidades obtidas\n",
    "        if pr_dado_f > pi_dado_f:\n",
    "            if (pr_dado_f > pi_dado_f):\n",
    "                #Se for verdade, na coluna classificador é colocada a informação de Relevante aquele tweet\n",
    "                dadosteste.loc[dadosteste.num==i,'Classificador'] = 'Relevante'\n",
    "        else:\n",
    "            #Se não for verdade, na coluna classificador é colocada a informação de Irrelevante aquele tweet\n",
    "            dadosteste.loc[dadosteste.num==i,'Classificador'] = 'Irrelevante'     \n",
    "            \n",
    "    except:\n",
    "        print(\"erro frase \")\n",
    "        print(frase)\n",
    "        print('')\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o comando crosstab, cruza-se as informações obtidas com o classificador e as informações reais. No caso, as informações reais é a classificação feita manualmente de tweets relevantes e irrelevantes na folha Teste do Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classificador</th>\n",
       "      <th>Irrelevante</th>\n",
       "      <th>Relevante</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevância</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Irrelevante</th>\n",
       "      <td>0.302013</td>\n",
       "      <td>0.194631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevante</th>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.463087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classificador  Irrelevante  Relevante\n",
       "relevância                           \n",
       "Irrelevante       0.302013   0.194631\n",
       "Relevante         0.040268   0.463087"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performaceteste = pd.crosstab(dadosteste['relevância'], dadosteste['Classificador'], normalize=True)\n",
    "performaceteste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente será feita a soma das porcetagens quando o Classificador acertou (bateu com a informação na coluna relevância), o que resulta em: **76,51% de acertos**. Um número rasoavelmente alto mesmo no cenário com novos tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o classificador desenvolvido pelo método de Naive-Bayes obteve-se resultados animadores mas ainda é uma classificação a cegas e tem seus contras. Por usar a frequência das palavras, termos repetidos durante a construção como artigos, pronomes, conjunções acabam possuindo um peso na probabilidade mas não estão diretamente ligadas ao tema como a palavra dólar, mercado financeiro, bolsa, entre outras. Ainda sobre probabilidade de palavras, o Naive-Bayes não consegue distinguir tweets com a mesma interpretação mesmo que escrita de outra forma (com termos menos frequentes), além de não distinguir frases com os termos trocados. Essa característica ocorre pois o Naive-Bayes interpreta quais são os termos que constituem o tweet e não a interpretação do tweet como um todo. Analisando no contexto macro do tema do projeto, esse classificador não se sairia tão bem em constantes análises de novos tweets. A coleta de dados para as planilhas Treinamento e Teste ocorreram no mesmo dia, por isso, os tópicos abordados foram muito próximos. Naquele dia da coleta em específico aconteceu alguns eventos marcantes que não são rotineiros como o circuit break e a alteração na bolsa de valores devido ao impacto epidemia do COVID-19 na economia mundial. Talvez esses tópicos são ocorram ao mesmo tempo daqui a um ano, então as palavras circuit break, COVID-19 e corona vírus serão descartáveis para determinar a relevância e dificultaria a performace do classificador. Mas o Naive-Bayes pode ser o método mais interessante para distinguir frases de diferentes línguas, modelos de algum produto (carros) pelos seus itens/características, como um sedan tem um grande porta-malas mas um 4x4 é mais robusto, aguenta ambientes com barro    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
